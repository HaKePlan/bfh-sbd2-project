---
title: "Group Work - Credit Scoring with R"
subtitle: "SBD2 - Data-driven Visualization and Decision-Making"
author: "Abdallah Abobaker, Severin Clauss, Dino Hamzic, Lenny Ruprecht"
date: Dez. 11, 2023
output:
    html_document:
        df_print: paged
        HTML: default
        toc: true
---

```{r include = FALSE, echo = FALSE}
# Install libraries needed
libraries <- c('readr', 'dlookr', 'dplyr', 'knitr', 'kableExtra', 'ggplot2', 'tidyr')

lapply(libraries, function(x) if (!(x %in% installed.packages())) {
  install.packages(x)
})

lapply(libraries, library, quietly = TRUE, character.only = TRUE)

if (basename(getwd()) != "bfh-sbd2-project") {
  setwd("./")
}

# read the dataset
data_improt <- read_csv('./loan_sample_7.csv')
loan_data <- data_improt
```
Check out on [GitHub](https://github.com/HaKePlan/bfh-sbd2-project)

***

# Exercise 1
*Using the data set "loan_data.csv", please go through the following tasks:*

## Descriptive analysis
### Structure
*Check and report the structure of the data set.*

```{r}
# TODO: wirte some intepretations?
# we start with checking the dimension, structure, head and tail of our dataset
dim(loan_data) # this shows us the demension of our dataset (40'000 Rows, 17 Cols)
str(loan_data) # structure of the dataset and its variables
head(loan_data)
tail(loan_data)

# we check the overview from dlookr to get a better understanding of the data quality
overview(loan_data)
overview <- overview(loan_data)
plot(overview)
```

### Categorical variables
*How many numeric and how many categorical variables are included in the data? What categorical variable has the most levels in it?*

As we know from [Structure `plot(overview)`](#structure) there are only two data types (character and numeric).
We guess that the character typed variables are categorical ones, but we need to verify this.
Thanks to `overview` we also know how many of each type exist, but we also want to show this clearly.


```{r}
# How many numeric and how many categorical?
overview |>
  filter(division == 'data type' & value > 0) |>
  select(metrics, value)

# from plot(overview) we know which are character variables
# lets check if each character variable is also a classifier
class_variables <- c('grade', 'home_ownership', 'verification_status', 'purpose', 'application_type')

result <- lapply(class_variables, function(x) loan_data |> distinct(across({{x}})))

knitr::kable(result) |> kable_styling(fixed_thead = TRUE)
```

As seen in `kable(result)` the `purpose` variable seems to have the most levels.
Also, we can clearly see that all character variables are categorical variables.
With this knowledge, we can say that our dataset consists of only two data types: numeric and categorical
(represented by character type).

### Summarize variables
*Summarize the variables. Discuss the summary statistics obtained.*
```{r}
knitr::kable(summary(loan_data)) |> kable_styling(fixed_thead = TRUE)
```

1. **loan_amnt**: The loan amount ranges from $1,000 to $40,000, with a mean (average) loan amount of approximately $11,661.
2. **int_rate**: The interest rate varies from 5.31% to 27.49%, with a mean interest rate of around 12.61%.
3. **grade**: This is a categorical variable representing the loan grade. The mode is not provided in the summary, but you can see the distribution of different grades.
4. **home_ownership**: Another categorical variable indicating the type of home ownership. The mode is not provided in the summary.
5. **annual_inc**: Annual income ranges from $6,600 to $400,000, with an average annual income of approximately $63,369.
6. **verification_status**: Categorical variable representing the status of income verification.
7. **purpose**: Categorical variable indicating the purpose of the loan.
8. **dti (debt-to-income ratio)**: The debt-to-income ratio ranges from 0 to 60.14, with a mean ratio of 18.24.
9. **open_acc**: The number of open credit lines ranges from 1 to 23, with a mean of approximately 10.3.
10. **revol_bal (revolving balance)**: The revolving balance on credit accounts ranges from 0 to $78,383, with a mean of approximately $11,946.
11. **revol_util (revolving utilization rate)**: The revolving utilization rate varies from 0% to 121.40%, with a mean of approximately 52.16%.
12. **total_acc (total number of credit lines)**: The total number of credit lines ranges from 3 to 57, with a mean of approximately 21.29.
13. **total_rec_int (total received interest)**: The total received interest on loans ranges from 0 to $8,834.9, with a mean of approximately $1,810.4.
14. **application_type**: Categorical variable indicating the type of loan application.
15. **tot_cur_bal (total current balance of all accounts)**: The total current balance across all accounts ranges from 0 to $472,573, with a mean of approximately $99,506.
16. **total_rev_hi_lim (total revolving credit limit)**: The total revolving credit limit ranges from $300 to $100,000, with a mean of approximately $24,122.
17. **Status**: Binary variable indicating the loan status, where 0 represents the "not defaulted" category and 1 represents "defaulted". The mean suggests that approximately 12.85% of loans fall into the "defaulted" category represented by 1.


### Target variable
*Check the levels of the target variable by choosing the appropriate visualization. Is the target variable balanced?*

```{r}
# since Status is still numerical, we need to convert it to an factor type
loan_data <- loan_data |> mutate(Status = as.factor(Status))

ggplot(loan_data, aes(x = Status, fill = Status)) +
  geom_bar() +
  ylab("Count") +
  xlab("Status of the loan") +
  scale_fill_manual(values = c('turquoise3', 'tomato'), labels = c("0 (Not Defaulted)", "1 (Defaulted)"))
```

The data set is **highly imbalanced**.

### Distribution of numeric variables
*Check the distribution of the numeric variables in the data set (include different visual representations).*

Since `Status` has to be considered a factor and not numerical, we will exclude it from this check:

```{r}
# Select numeric variables
numeric_vars <- loan_data %>% select_if(is.numeric)

# Gather the data for plotting
gathered_data <- gather(numeric_vars, key = "Variable", value = "Value")

# Plot the distribution of numeric variables
ggplot(gathered_data, aes(x = Value, fill = Variable)) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~ Variable, scales = "free") +
  labs(title = "Distribution of Numeric Variables",
       x = "",
       y = "") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Checking for outliers
*Investigate whether certain variables contain outliers (hint: what does a box plot show?).*

```{r}
# to gather infromation over the outliers, we need to exclude the character and factor variables
# Since we did this before, we can just use the prepared variables (numeric_vars and gathered_data)

ggplot(gathered_data, aes(x = Value, fill = Variable)) +
  geom_boxplot(show.legend = FALSE) +
  facet_wrap(~ Variable, scales = "free") +
  labs(title = "Distribution of Numeric Variables",
       x = "",
       y = "") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# lets also take a look at the bare numbers with diagonse
diagnose_outlier(numeric_vars)
```

The first overview gives us the impression that we need to investigate further into the outliers.
Therefore, we want to exactly identify where the outliers are problematic and interfere.
Let's get a comparison between the variables cleaned from outliers and raw:

```{r}
numeric_vars |>
  plot_outlier(
    diagnose_outlier(numeric_vars) |>
      filter(outliers_ratio >= 0.5) |>
      select(variables) |>
      unlist()
  )
```

### Dealing with outliers
*Elaborate your view on how to proceed in dealing with the outliers and â€“ if necessary, take appropriate action.*

## Distribution of numeric features in target
*Choose the appropriate visualization to investigate the distribution of the numeric features per the two levels of our target feature (i.e. default vs non-default). Discuss the visualizations. Which variables seem relevant in predicting the target feature?*

## categorical variables and target
*Use a bar plot visualization to investigate the associations between the categorical variables and the target feature.*

## Correlations
*Visualize the correlations that emerge between the numerical features. Discuss the results. Which variables are highly correlated? Decide whether you keep all variables.*

## Association between loan and income
*Plot an interactive scatter plot of the association between the loan amount requested and the annual income of the borrower. Discuss the plot. What can you tell about the association?*

## Create a new balanced data set
*Create a new balanced data set where the two levels of the target variable will be equally represented; Create a bar plot of the newly created target variable. Why is this step necessary?*

***

# Exercise 2
*Using the new balanced data set:*

## Train and test a logistic classifier
### Divide the sample
*Divide the sample into training and testing set using 70% for training the algorithm.*

### Train the classifier
*Train the classifier and report the coefficients obtained and interpret the results.*

### Plot the ROC
*Plot the ROC and the Precision/Recall Curve and interpret the results.*

### Confusion matrix
*Produce the confusion matrix and interpret the results.*

### AUC values
*Report the AUC values and the overall accuracy and interpret the results.*

***

# Exercise 3
*Thinking about the pre-processing steps that you carried out before training the logistic classifier:*

## Predictive performance
*Can you think of a way to improve the predictive performance of your data? What can you do differently? (Hint: Feel free to be creative and discuss any additional step in data collection and/or data pre-processing that you might try so to improve the results)*

***

# Exercise 4
*Finally, thinking about putting your model into action and basing credit decisions on the prediction that it generates:*

## Challenges while using this model
*What kind of challenges may a company face if it uses your model in their daily business, in particular in regard to ethical challenges and moral obligations companies have? Please refer to the "common ethical issues in the context the creation of value from data" (see slides week 11) in your answer.*

## Mitigate the issue
*Can you think of a way how companies can overcome or at least mitigate the issues that you described above?*
