---
title: "Group Work - Credit Scoring with R"
subtitle: "SBD2 - Data-driven Visualization and Decision-Making"
author: "Abdallah Abobaker, Severin Clauss, Dino Hamzic, Lenny Ruprecht"
date: Dez. 11, 2023
output:
    html_document:
        df_print: paged
        HTML: default
        toc: true
---

```{r include = FALSE, echo = FALSE}
# Install libraries needed
libraries <- c('readr', 'dlookr', 'dplyr', 'knitr', 'kableExtra', 'ggplot2', 'tidyr', 'purrr', 'Boruta', 'ggcorrplot', 'plotly', 'ROSE', 'caret', 'DescTools', 'ROCR')

lapply(libraries, function(x) if (!(x %in% installed.packages())) {
  install.packages(x)
})

lapply(libraries, library, quietly = TRUE, character.only = TRUE)

if (basename(getwd()) != "bfh-sbd2-project") {
  setwd("./")
}

# read the dataset
data_improt <- read_csv('./loan_sample_7.csv')
loan_data <- data_improt
```
Check out on [GitHub](https://github.com/HaKePlan/bfh-sbd2-project)

***

# Exercise 1
*Using the data set "loan_data.csv", please go through the following tasks:*

## Descriptive analysis
### Structure
*Check and report the structure of the data set.*

```{r}
# TODO: wirte some intepretations?
# we start with checking the dimension, structure, head and tail of our dataset
dim(loan_data) # this shows us the demension of our dataset (40'000 Rows, 17 Cols)
```
### Dimension:
- We can see that the dataset has 40'000 individual records and 17 different variables.

```{r}
str(loan_data) # structure of the dataset and its variables
```
### Structure:
- In the structure of the dataset we can see, that it provides us with a well balanced set of variables, that all could be used in order to predict our target.
```{r}
head(loan_data)
tail(loan_data)

# we check the overview from dlookr to get a better understanding of the data quality
overview(loan_data)
overview <- overview(loan_data)
plot(overview)
```

As we see, the Status is considered numeric and not a factor. Let's change that!
Also, we want to sort the data set a bit and put all non-numerical variables at the end of the dataset for later use.

```{r}
loan_data <- loan_data |> mutate(Status = as.factor(Status))

numerci_loan <- loan_data[sapply(loan_data, is.numeric)]
non_numeric_loan <- loan_data[sapply(loan_data, function(x) !is.numeric(x))]
loan_data <- cbind(numerci_loan, non_numeric_loan)

# let us show the new overview plot
overview <- overview(loan_data)
plot(overview)
```

Now that looks better, cleaner and is simpler to understand.

### Categorical variables
*How many numeric and how many categorical variables are included in the data? What categorical variable has the most levels in it?*

As we know from [Structure `plot(overview)`](#structure) there are only two data types (character and numeric).
We guess that the character typed variables are categorical ones, but we need to verify this.
Thanks to `overview` we also know how many of each type exist, but we also want to show this clearly.


```{r}
# How many numeric and how many categorical?
overview |>
  filter(division == 'data type' & value > 0) |>
  select(metrics, value)

# from plot(overview) we know which are character variables
# lets check if each character variable is also a classifier
class_variables <- c('grade', 'home_ownership', 'verification_status', 'purpose', 'application_type')

result <- lapply(class_variables, function(x) loan_data |> distinct(across({{x}})))

knitr::kable(result) |> kable_styling(fixed_thead = TRUE)
```

As seen in `kable(result)` the `purpose` variable seems to have the most levels.
Also, we can clearly see that all character variables are categorical variables.
With this knowledge, we can say that our dataset consists of only two data types: numeric and categorical
(represented by character type).

### Summarize variables
*Summarize the variables. Discuss the summary statistics obtained.*
```{r}
knitr::kable(summary(loan_data)) |> kable_styling(fixed_thead = TRUE)
```

1. **loan_amnt**: The loan amount ranges from $1,000 to $40,000, with a mean (average) loan amount of approximately $11,661.
2. **int_rate**: The interest rate varies from 5.31% to 27.49%, with a mean interest rate of around 12.61%.
3. **grade**: This is a categorical variable representing the loan grade. The mode is not provided in the summary, but you can see the distribution of different grades.
4. **home_ownership**: Another categorical variable indicating the type of home ownership. The mode is not provided in the summary.
5. **annual_inc**: Annual income ranges from $6,600 to $400,000, with an average annual income of approximately $63,369.
6. **verification_status**: Categorical variable representing the status of income verification.
7. **purpose**: Categorical variable indicating the purpose of the loan.
8. **dti (debt-to-income ratio)**: The debt-to-income ratio ranges from 0 to 60.14, with a mean ratio of 18.24.
9. **open_acc**: The number of open credit lines ranges from 1 to 23, with a mean of approximately 10.3.
10. **revol_bal (revolving balance)**: The revolving balance on credit accounts ranges from 0 to $78,383, with a mean of approximately $11,946.
11. **revol_util (revolving utilization rate)**: The revolving utilization rate varies from 0% to 121.40%, with a mean of approximately 52.16%.
12. **total_acc (total number of credit lines)**: The total number of credit lines ranges from 3 to 57, with a mean of approximately 21.29.
13. **total_rec_int (total received interest)**: The total received interest on loans ranges from 0 to $8,834.9, with a mean of approximately $1,810.4.
14. **application_type**: Categorical variable indicating the type of loan application.
15. **tot_cur_bal (total current balance of all accounts)**: The total current balance across all accounts ranges from 0 to $472,573, with a mean of approximately $99,506.
16. **total_rev_hi_lim (total revolving credit limit)**: The total revolving credit limit ranges from $300 to $100,000, with a mean of approximately $24,122.
17. **Status**: Binary variable indicating the loan status, where 0 represents the "not defaulted" category and 1 represents "defaulted". The mean suggests that approximately 12.85% of loans fall into the "defaulted" category represented by 1.


### Target variable
*Check the levels of the target variable by choosing the appropriate visualization. Is the target variable balanced?*

```{r}
# since Status is still numerical, we need to convert it to an factor type
loan_data <- loan_data |> mutate(Status = as.factor(Status))

ggplot(loan_data, aes(x = Status, fill = Status)) +
  geom_bar() +
  ylab("Count") +
  xlab("Status of the loan") +
  scale_fill_manual(values = c('turquoise3', 'tomato'), labels = c("0 (Not Defaulted)", "1 (Defaulted)"))
```

The data set is **highly imbalanced**.

### Distribution of numeric variables
*Check the distribution of the numeric variables in the data set (include different visual representations).*

Since `Status` has to be considered a factor and not numerical, we will exclude it from this check:

```{r}
# Select numeric variables
numeric_vars <- loan_data %>% select_if(is.numeric)

# Gather the data for plotting
gathered_data <- gather(numeric_vars, key = "Variable", value = "Value")

# Plot the distribution of numeric variables
ggplot(gathered_data, aes(x = Value, fill = Variable)) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~ Variable, scales = "free") +
  labs(title = "Distribution of Numeric Variables",
       x = "",
       y = "") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Checking for outliers
*Investigate whether certain variables contain outliers (hint: what does a box plot show?).*

```{r}
# to gather infromation over the outliers, we need to exclude the character and factor variables
# Since we did this before, we can just use the prepared variables (numeric_vars and gathered_data)

ggplot(gathered_data, aes(x = Value, fill = Variable)) +
  geom_boxplot(show.legend = FALSE) +
  facet_wrap(~ Variable, scales = "free") +
  labs(title = "Distribution of Numeric Variables",
       x = "",
       y = "") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# lets also take a look at the bare numbers with diagonse
diagnose_outlier(numeric_vars)
```

The first overview gives us the impression that we need to investigate further into the outliers.
Therefore, we want to exactly identify where the outliers are problematic and interfere.
Let's get a comparison between the variables cleaned from outliers and raw:

```{r}
numeric_vars |>
  plot_outlier(
    diagnose_outlier(numeric_vars) |>
      filter(outliers_ratio >= 0.5) |>
      select(variables) |>
      unlist()
  )
```

### Dealing with outliers
*Elaborate your view on how to proceed in dealing with the outliers and â€“ if necessary, take appropriate action.*

```{r}
outlier <- function(x) {
  quantiles <- quantile(x, c(.05, .95))
  x[x < quantiles[1]] <- quantiles[1]
  x[x > quantiles[2]] <- quantiles[2]
  x
}

clean_loan_data <- map_df(loan_data[,-c(11:17)], outlier)
cols <- loan_data[,c(11:17)]
clean_loan_data <- cbind(clean_loan_data, cols)

boxplot(scale(clean_loan_data[,c(1:10)]), use.cols = TRUE)
```

## Distribution of numeric features in target
*Choose the appropriate visualization to investigate the distribution of the numeric features per the two levels of our target feature (i.e. default vs non-default). Discuss the visualizations. Which variables seem relevant in predicting the target feature?*

```{r}
for (i in 1:length(clean_loan_data[,-c(11:17)])) {
  print(
    ggplot(clean_loan_data, aes(y = clean_loan_data[,i], color = Status)) +
      geom_boxplot() +
      ylab(names(clean_loan_data[i])) +
      scale_color_manual(values = c('turquoise3', 'tomato'), labels = c("0 (Not Defaulted)", "1 (Defaulted)")) +
      theme(
        axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()
      )
  )
}
```

## categorical variables and target
*Use a bar plot visualization to investigate the associations between the categorical variables and the target feature.*

```{r}
categorical_columns <- names(clean_loan_data)[12:16]

for (cat_col in categorical_columns) {
  print(
    clean_loan_data |>
      group_by({{cat_col}}, Status) |>
      summarise(count = n()) |>
      ggplot(aes(x = {{cat_col}}, y = count, fill = factor(Status))) +
      geom_bar(stat = "identity", position = "dodge") +
      labs(
        title = paste("Association between", cat_col, "and Status"),
        x = cat_col,
        y = "Count"
      ) +
      scale_fill_manual(
        values = c('turquoise3', 'tomato'),
        labels = c("0 (Not Defaulted)", "1 (Defaulted)")
      ) +
      theme_minimal()
  )
}
```

## Correlations
*Visualize the correlations that emerge between the numerical features. Discuss the results. Which variables are highly correlated? Decide whether you keep all variables.*

```{r}
correlations <- cor(clean_loan_data[-c(11:17)])
p_value_mat <- cor_pmat(clean_loan_data[,-c(11:17)])
ggcorrplot(correlations, type = "lower", p.mat = p_value_mat)
```

To get a better understanding to decide whether to keep all variables or remove some,
we also want to use the Boruta algorithm.

Run the Boruta algo and print significant attributes:
```{r}
boruta_output <- Boruta(Status~., data = clean_loan_data)
boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
print(boruta_signif)
```

Visualize the results:
```{r}
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")
```

Based on the correlation plot and on the Boruta result, we do not see any variable to remove.

## Association between loan and income
*Plot an interactive scatter plot of the association between the loan amount requested and the annual income of the borrower. Discuss the plot. What can you tell about the association?*

```{r}
ggplotly(
  ggplot(clean_loan_data, aes(x = loan_amnt, y = annual_inc)) +
    geom_point(
      colour = "cornflowerblue",
      alpha = 0.5
    ) +
    labs(
      title = "Scatter Plot of Loan Amount vs. Annual Income (handeld outliers)",
      x = "Loan Amount",
      y = "Annual Income"
    )
)
```

It is hard to see a clear picture, but at least we can visually see a tendencies for a positive correlation.
But maybe the same plot with the data including outliers can give us a better understanding,
since in `clean_loan_data` we have [replaced the outliers](#dealing-with-outliers) with values from the quantiles
(hence, the box like appearance).

```{r}
ggplotly(
  ggplot(loan_data, aes(x = loan_amnt, y = annual_inc)) +
    geom_point(
      colour = "cornflowerblue",
      alpha = 0.5
    ) +
    labs(
      title = "Scatter Plot of Loan Amount vs. Annual Income (including outliers)",
      x = "Loan Amount",
      y = "Annual Income"
    )
)
```

Now here we can visually see the positive correlation even more.

Both of them show us a mostly clear line of a maximum requested loan amount with a certain annual income.
Also, the comparison between the one with handled outliers and the one with unhandled outliers raises a question
if it was correct to remove the outliers for `loan_amnt` and `annual_inc`.
We decided to keep them handled and revalidate it later.

## Create a new balanced data set
*Create a new balanced data set where the two levels of the target variable will be equally represented; Create a bar plot of the newly created target variable. Why is this step necessary?*

```{r}
set.seed(7)
data_balanced <- ovun.sample(Status ~ ., data=clean_loan_data, method = "under")
balanced_loan_data <- data.frame(data_balanced[["data"]])

ggplot(balanced_loan_data, aes(x = Status, fill = Status)) +
  geom_bar() +
  ylab("Count") +
  xlab("Status of the loan") +
  scale_fill_manual(values = c('skyblue', 'brown2'), labels = c("0 (Not Defaulted)", "1 (Defaulted)"))
```

This step was necessary to get a balanced dataset of our target variable to later train our prediction model.
Before we had a very unbalanced dataset.

***

# Exercise 2
*Using the new balanced data set:*

## Train and test a logistic classifier
### Divide the sample
*Divide the sample into training and testing set using 70% for training the algorithm.*

```{r}
set.seed(7)
div <- createDataPartition(y = balanced_loan_data$Status, p = 0.7, list = F)

# Training Sample
data.train <- balanced_loan_data[div,] # 70% here
PercTable(data.train$Status)

# Test Sample
data.test <- balanced_loan_data[-div,] # rest of the 30% data goes here
PercTable(data.test$Status)
```

### Train the classifier
*Train the classifier and report the coefficients obtained and interpret the results.*

```{r}
default_model_lr1 <- glm(Status~., data = data.train, family = binomial())

summary(default_model_lr1)
```
We can clearly see which variables have a significant influence on the model and which not.

This print shows us a short overview over the significant (with p-value lower than 0.05).

```{r, echo=FALSE}
significant.variables <- summary(default_model_lr1)$coeff[-1,4] < 0.05
names(significant.variables)[significant.variables == TRUE]
```

We notice that 11 out of 17 are significant.
We need to note,
that in the class variables of `grade`, `home_ownership`, `purpose`, and `verification_status`
only a some classes are considered significant:

- **Grade:** B and C
- **home_ownership:** RENT
- **purpose:** small_business
- **verification_status:** Source Verified

### Plot the ROC
*Plot the ROC and the Precision/Recall Curve and interpret the results.*

```{r}
data.test$default_model_lr1_score <- predict(default_model_lr1, type='response', data.test)
default_model_lr1_pred <- prediction(data.test$default_model_lr1_score, data.test$Status)
default_model_lr1_roc <- performance(default_model_lr1_pred, "tpr", "fpr")

plot(default_model_lr1_roc, lwd=1, colorize = TRUE, main = "default_model_lr1: Logit - ROC Curve")
lines(x=c(0, 1), y=c(0, 1), col="black", lwd=1, lty=3)
```

The curve seems a bit flattened, and therefore the "Area under the curve" (AUC) small.
At first sight, we think it is not ready for using it in production.

```{r}
default_model_lr1_precision <- performance(default_model_lr1_pred, measure = "prec", x.measure = "rec")
plot(default_model_lr1_precision, main="Fit1: Logit - Precision vs Recall")
```

### Confusion matrix
*Produce the confusion matrix and interpret the results.*

```{r, echo=FALSE}
confusionMatrix(as.factor(round(data.test$default_model_lr1_score)), data.test$Status)
```

1. **Accuracy:** The model's overall accuracy is 63.71%, indicating that it correctly classifies about 63.71% of instances.
2. **Sensitivity and Specificity:** Both sensitivity and specificity are similar, suggesting that the model performs reasonably well in predicting both positive and negative instances.
3. **Precision and Negative Predictive Value:** Precision (positive predictive value) and negative predictive value are also quite balanced, indicating a reasonable trade-off between correctly identifying positive and negative instances.
4. **Prevalence:** The dataset has a prevalence of 49.89%, indicating that the positive class is slightly underrepresented.
5. **Kappa:** The Kappa value of 0.2741 suggests fair agreement beyond random chance. While it's not a high agreement, it's still indicative of some level of predictive power.

In summary, the model shows a moderate level of performance, and further tuning or exploration may be needed.

### AUC values
*Report the AUC values and the overall accuracy and interpret the results.*

```{r, echo=FALSE}
default_model_lr1_auc <- performance(default_model_lr1_pred, measure = "auc")
cat("AUC: ", default_model_lr1_auc@y.values[[1]]*100)
```

As stated in the interpretation of [Confusion matrix](#confusion-matrix),
the accuracy is not very high and indicates a moderate level of performance.

***

# Exercise 3
*Thinking about the pre-processing steps that you carried out before training the logistic classifier:*

## Predictive performance
*Can you think of a way to improve the predictive performance of your data? What can you do differently? (Hint: Feel free to be creative and discuss any additional step in data collection and/or data pre-processing that you might try so to improve the results)*

***

# Exercise 4
*Finally, thinking about putting your model into action and basing credit decisions on the prediction that it generates:*

## Challenges while using this model
*What kind of challenges may a company face if it uses your model in their daily business, in particular in regard to ethical challenges and moral obligations companies have? Please refer to the "common ethical issues in the context the creation of value from data" (see slides week 11) in your answer.*

## Mitigate the issue
*Can you think of a way how companies can overcome or at least mitigate the issues that you described above?*
